{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T16:26:41.932901Z",
     "iopub.status.busy": "2025-12-07T16:26:41.932609Z",
     "iopub.status.idle": "2025-12-07T16:26:51.855973Z",
     "shell.execute_reply": "2025-12-07T16:26:51.854975Z",
     "shell.execute_reply.started": "2025-12-07T16:26:41.932867Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install ffmpeg-python pillow\n",
    "!git clone https://github.com/soCzech/TransNetV2.git\n",
    "%cd TransNetV2/inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T16:26:51.857591Z",
     "iopub.status.busy": "2025-12-07T16:26:51.85729Z",
     "iopub.status.idle": "2025-12-07T16:26:56.31853Z",
     "shell.execute_reply": "2025-12-07T16:26:56.317559Z",
     "shell.execute_reply.started": "2025-12-07T16:26:51.857557Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import glob\n",
    "import torch\n",
    "import ffmpeg\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from transnetv2 import TransNetV2\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T16:26:56.321016Z",
     "iopub.status.busy": "2025-12-07T16:26:56.319948Z",
     "iopub.status.idle": "2025-12-07T16:26:56.339651Z",
     "shell.execute_reply": "2025-12-07T16:26:56.338898Z",
     "shell.execute_reply.started": "2025-12-07T16:26:56.320972Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Defined input directories\n",
    "input_dirs = [\n",
    "    '/kaggle/input/aic2024-videos-part1-1', # Contains L01 to L06\n",
    "    # '/kaggle/input/aic2024-videos-part1'     # Contains L07 to L12\n",
    "]\n",
    "\n",
    "all_video_paths = dict()\n",
    "\n",
    "# Iterate through both directories\n",
    "for videos_dir in input_dirs:\n",
    "    if not os.path.exists(videos_dir):\n",
    "        print(f\"Directory not found: {videos_dir}\")\n",
    "        continue\n",
    "        \n",
    "    for part in sorted(os.listdir(videos_dir)):\n",
    "        # Check if folder name matches \"Videos_Lxx\" pattern\n",
    "        if not part.startswith(\"Videos_\"):\n",
    "            continue\n",
    "            \n",
    "        data_part = part.split('_')[-1] # Extracts L01, L02...\n",
    "        \n",
    "        # Initialize dictionary for this part if not exists\n",
    "        if data_part not in all_video_paths:\n",
    "            all_video_paths[data_part] = dict()\n",
    "\n",
    "        data_part_path = f'{videos_dir}/Videos_{data_part}/video'\n",
    "        \n",
    "        if not os.path.exists(data_part_path):\n",
    "            continue\n",
    "            \n",
    "        video_paths = sorted(os.listdir(data_part_path))\n",
    "        # Filter for mp4 only to be safe\n",
    "        video_ids = [vp.replace('.mp4', '').split('_')[-1] for vp in video_paths if vp.endswith('.mp4')]\n",
    "        \n",
    "        for video_id, video_path in zip(video_ids, video_paths):\n",
    "            if not video_path.endswith('.mp4'): continue\n",
    "            video_path_full = f'{data_part_path}/{video_path}'\n",
    "            all_video_paths[data_part][video_id] = video_path_full\n",
    "\n",
    "print(f\"Total Parts Found: {sorted(all_video_paths.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T16:26:56.340845Z",
     "iopub.status.busy": "2025-12-07T16:26:56.340544Z",
     "iopub.status.idle": "2025-12-07T16:26:56.349454Z",
     "shell.execute_reply": "2025-12-07T16:26:56.348571Z",
     "shell.execute_reply.started": "2025-12-07T16:26:56.340819Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "num_batch = 1\n",
    "BATCH_ID = 0\n",
    "MEMBER_ID = 0    \n",
    "num_member = 1   # Ch·∫°y 100% video\n",
    "\n",
    "all_videos = [x for v in all_video_paths.values() for x in v.values()]\n",
    "\n",
    "# Safety check\n",
    "if len(all_videos) > 0:\n",
    "    # T√≠nh s·ªë l∆∞·ª£ng video m·ªói member c·∫ßn l√†m\n",
    "    # D√πng math.ceil ƒë·ªÉ ƒë·∫£m b·∫£o chia h·∫øt ho·∫∑c d∆∞ v√†o batch cu·ªëi\n",
    "    import math\n",
    "    batch_len = math.ceil(len(all_videos) / num_batch / num_member)\n",
    "else:\n",
    "    batch_len = 0\n",
    "\n",
    "all_batches_info = {n: {} for n in range(num_batch)}\n",
    "current_idx = 0\n",
    "\n",
    "for n in range(num_batch):\n",
    "    for m in range(num_member):\n",
    "        start = current_idx\n",
    "        end = current_idx + batch_len\n",
    "        \n",
    "        # ƒê·∫£m b·∫£o kh√¥ng v∆∞·ª£t qu√° t·ªïng s·ªë video\n",
    "        if end > len(all_videos):\n",
    "            end = len(all_videos)\n",
    "            \n",
    "        # --- FIX QUAN TR·ªåNG T·∫†I ƒê√ÇY ---\n",
    "        # Lu√¥n g√°n v√†o dict con [m], KH√îNG BAO GI·ªú g√°n tr·ª±c ti·∫øp list v√†o [n]\n",
    "        # Ngay c·∫£ khi num_member = 1, ta v·∫´n d√πng key [0]\n",
    "        all_batches_info[n][m] = all_videos[start:end]\n",
    "        \n",
    "        current_idx = end\n",
    "        \n",
    "# Debug: Ki·ªÉm tra xem n√≥ c√≥ ph·∫£i l√† List kh√¥ng\n",
    "print(f\"Type check: {type(all_batches_info[BATCH_ID][MEMBER_ID])}\") \n",
    "# N√≥ ph·∫£i in ra <class 'list'> th√¨ m·ªõi ƒë√∫ng. Tr∆∞·ªõc ƒë√≥ n√≥ in ra <class 'str'> n√™n m·ªõi l·ªói.\n",
    "\n",
    "with open(\"/kaggle/working/batch_info.json\", 'w') as f:\n",
    "    # Convert keys to str for JSON serialization if needed, though int keys are coerced\n",
    "    json.dump(all_batches_info, f)\n",
    "    \n",
    "print(f\"Total videos to process: {len(all_videos)}\")\n",
    "print(f\"Videos assigned to this worker: {len(all_batches_info[BATCH_ID][MEMBER_ID])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T16:26:56.351415Z",
     "iopub.status.busy": "2025-12-07T16:26:56.350655Z",
     "iopub.status.idle": "2025-12-07T16:27:00.989442Z",
     "shell.execute_reply": "2025-12-07T16:27:00.98875Z",
     "shell.execute_reply.started": "2025-12-07T16:26:56.351377Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = TransNetV2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": false,
    "execution": {
     "iopub.execute_input": "2025-12-07T16:27:00.99105Z",
     "iopub.status.busy": "2025-12-07T16:27:00.990672Z",
     "iopub.status.idle": "2025-12-07T16:28:17.617803Z",
     "shell.execute_reply": "2025-12-07T16:28:17.616901Z",
     "shell.execute_reply.started": "2025-12-07T16:27:00.991011Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import os\n",
    "import json\n",
    "\n",
    "save_dir = '/kaggle/working/scenes'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# L·∫•y danh s√°ch video t·ª´ config ·ªü Cell 4\n",
    "videos_to_process = all_batches_info[BATCH_ID][MEMBER_ID]\n",
    "print(f\"üöÄ Start processing {len(videos_to_process)} videos for Scene Detection...\")\n",
    "\n",
    "count = 0\n",
    "for i, video_path in enumerate(videos_to_process):\n",
    "    try:\n",
    "        # --- LOGIC FIX L·ªñI T√äN FILE ---\n",
    "        filename = video_path.split('/')[-1]\n",
    "        parts = filename.split('_')\n",
    "        \n",
    "        if len(parts) >= 2:\n",
    "            # Chu·∫©n: L01_V001.mp4 -> Batch: L01\n",
    "            video_batch = parts[0]\n",
    "            video_name = \"_\".join(parts[1:])\n",
    "        else:\n",
    "            # L·ªói: test.mp4 -> L·∫•y Batch t·ª´ t√™n th∆∞ m·ª•c cha (Videos_L01)\n",
    "            parent_dir = video_path.split('/')[-3] \n",
    "            if \"Videos_\" in parent_dir:\n",
    "                video_batch = parent_dir.split('_')[-1]\n",
    "            else:\n",
    "                video_batch = \"Uncategorized\"\n",
    "            video_name = filename\n",
    "\n",
    "        video_name = video_name.replace('.mp4', '')\n",
    "        \n",
    "        # T·∫°o th∆∞ m·ª•c con: /scenes/L01\n",
    "        batch_save_dir = os.path.join(save_dir, video_batch)\n",
    "        os.makedirs(batch_save_dir, exist_ok=True)\n",
    "        \n",
    "        json_path = f\"{batch_save_dir}/{video_name}.json\"\n",
    "        \n",
    "        # N·∫øu file ƒë√£ c√≥ r·ªìi th√¨ b·ªè qua (ƒë·ªÉ resume n·∫øu b·ªã ng·∫Øt)\n",
    "        if os.path.exists(json_path):\n",
    "            continue\n",
    "\n",
    "        # --- G·ªåI TRANSNET MODEL ---\n",
    "        _, single_frame_predictions, _ = model.predict_video(video_path)\n",
    "        scenes = model.predictions_to_scenes(single_frame_predictions)\n",
    "        \n",
    "        with open(json_path, 'w') as f:\n",
    "            json.dump(scenes.tolist(), f)\n",
    "            \n",
    "        count += 1\n",
    "        if count % 10 == 0:\n",
    "            print(f\"   ...Processed {count}/{len(videos_to_process)} scenes.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error on {video_path}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"‚úÖ Scene Detection FINISHED! Created {count} json files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T16:28:17.619899Z",
     "iopub.status.busy": "2025-12-07T16:28:17.619179Z",
     "iopub.status.idle": "2025-12-07T16:28:17.626509Z",
     "shell.execute_reply": "2025-12-07T16:28:17.625513Z",
     "shell.execute_reply.started": "2025-12-07T16:28:17.619855Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# CELL 7: Optimized Save Frames with Compression\n",
    "def save_frames(video_path: str, frame_numbers: np.ndarray, save_dir: str):\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    frame_numbers = np.sort(np.unique(frame_numbers))\n",
    "    \n",
    "    frame_idx = 0\n",
    "    if len(frame_numbers) == 0:\n",
    "        video.release()\n",
    "        return\n",
    "\n",
    "    frame_it = frame_numbers[frame_idx]\n",
    "    max_frame = frame_numbers[-1] + 1\n",
    "    \n",
    "    for i in range(max_frame):\n",
    "        ret, frame = video.read()       \n",
    "        if not ret: break\n",
    "            \n",
    "        if i == frame_it:\n",
    "            filename = \"{}/{:0>4d}.jpg\".format(f'{save_dir}', i)\n",
    "            # N√©n ·∫£nh JPG ch·∫•t l∆∞·ª£ng 80-85 ƒë·ªÉ ti·∫øt ki·ªám dung l∆∞·ª£ng\n",
    "            cv2.imwrite(filename, frame, [int(cv2.IMWRITE_JPEG_QUALITY), 80])\n",
    "            \n",
    "            frame_idx += 1\n",
    "            if frame_idx < len(frame_numbers):\n",
    "                frame_it = frame_numbers[frame_idx]\n",
    "            else:\n",
    "                break\n",
    "    video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-12-07T16:28:18.416Z",
     "iopub.execute_input": "2025-12-07T16:28:17.628207Z",
     "iopub.status.busy": "2025-12-07T16:28:17.627874Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- CELL 8: Split Strategy & Safe Direct-Write ---\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import cv2\n",
    "import os\n",
    "import json\n",
    "import zipfile\n",
    "import shutil\n",
    "import glob\n",
    "\n",
    "# ==========================================\n",
    "# ‚öôÔ∏è C·∫§U H√åNH BATCH C·∫¶N CH·∫†Y T·∫†I ƒê√ÇY\n",
    "# ==========================================\n",
    "# Run 1: Ch·∫°y L01 -> L05\n",
    "TARGET_BATCHES = ['L01', 'L02', 'L03', 'L04', 'L05']\n",
    "OUTPUT_ZIP_NAME = 'keyframes_part1_set1.zip'\n",
    "\n",
    "# Run 2 (Notebook kh√°c ho·∫∑c l·∫ßn ch·∫°y sau): Uncomment d√≤ng d∆∞·ªõi\n",
    "# TARGET_BATCHES = ['L06'] \n",
    "# OUTPUT_ZIP_NAME = 'keyframes_part1_set2.zip'\n",
    "# ==========================================\n",
    "\n",
    "# ƒê∆∞·ªùng d·∫´n l√†m vi·ªác\n",
    "work_dir = '/kaggle/working'\n",
    "scene_json_dirs = '/kaggle/working/scenes'\n",
    "final_zip_path = os.path.join(work_dir, OUTPUT_ZIP_NAME)\n",
    "\n",
    "# D·ªçn d·∫πp file c≈© n·∫øu c√≥ ƒë·ªÉ tr√°nh l·ªói append\n",
    "if os.path.exists(final_zip_path):\n",
    "    print(f\"‚ö†Ô∏è Found existing {OUTPUT_ZIP_NAME}, removing to start fresh...\")\n",
    "    os.remove(final_zip_path)\n",
    "\n",
    "# --- H√ÄM CHI·∫æN L∆Ø·ª¢C FRAME (GI·ªÆ NGUY√äN) ---\n",
    "def get_adaptive_frames(scenes):\n",
    "    frames_to_capture = []\n",
    "    for start, end in scenes:\n",
    "        duration = end - start\n",
    "        if duration <= 1: continue \n",
    "        if duration < 25: \n",
    "            frames_to_capture.append((start + end) // 2)\n",
    "        elif duration < 150: \n",
    "            frames_to_capture.extend([start, (start + end) // 2, end - 1])\n",
    "        else: \n",
    "            step = 50 \n",
    "            sampled = range(start, end, step)\n",
    "            frames_to_capture.extend(sampled)\n",
    "            if (end - 1) not in frames_to_capture:\n",
    "                frames_to_capture.append(end - 1)\n",
    "    return sorted(list(set(frames_to_capture)))\n",
    "\n",
    "# --- H√ÄM KI·ªÇM TRA DUNG L∆Ø·ª¢NG TR·ªêNG ---\n",
    "def get_free_space_gb():\n",
    "    total, used, free = shutil.disk_usage(work_dir)\n",
    "    return free / (1024**3)\n",
    "\n",
    "# --- V√íNG L·∫∂P CH√çNH ---\n",
    "print(f\"üöÄ Starting Extraction for Batches: {TARGET_BATCHES}\")\n",
    "print(f\"üíæ Saving to: {OUTPUT_ZIP_NAME}\")\n",
    "\n",
    "# M·ªü file Zip M·ªòT L·∫¶N DUY NH·∫§T ·ªü ch·∫ø ƒë·ªô 'w'\n",
    "# ƒêi·ªÅu n√†y gi√∫p t·ªëi ∆∞u I/O v√† tr√°nh l·ªói ph√¢n m·∫£nh file\n",
    "with zipfile.ZipFile(final_zip_path, 'w', zipfile.ZIP_DEFLATED) as zf:\n",
    "    \n",
    "    stop_processing = False\n",
    "    \n",
    "    for key in TARGET_BATCHES:\n",
    "        if stop_processing: break\n",
    "        \n",
    "        # Ki·ªÉm tra xem Batch n√†y c√≥ trong d·ªØ li·ªáu ƒë·∫ßu v√†o kh√¥ng\n",
    "        if key not in all_video_paths:\n",
    "            print(f\"‚ö†Ô∏è Batch {key} not found in input sources. Skipping.\")\n",
    "            continue\n",
    "            \n",
    "        video_paths_dict = all_video_paths[key]\n",
    "        video_ids = sorted(video_paths_dict.keys())\n",
    "        \n",
    "        print(f\"üëâ Processing Batch {key} ({len(video_ids)} videos)...\")\n",
    "        \n",
    "        # Progress bar cho t·ª´ng video\n",
    "        pbar = tqdm(video_ids, desc=f\"Zipping {key}\", unit=\"vid\")\n",
    "        \n",
    "        for video_id in pbar:\n",
    "            # --- SAFETY CHECK: D·ª™NG N·∫æU ·ªî C·ª®NG S·∫ÆP ƒê·∫¶Y (< 1GB) ---\n",
    "            if get_free_space_gb() < 1.0:\n",
    "                print(f\"\\nüõë CRITICAL WARNING: Disk space low ({get_free_space_gb():.2f} GB left).\")\n",
    "                print(\"üõë Stopping gracefully to save current Zip file.\")\n",
    "                stop_processing = True\n",
    "                break\n",
    "\n",
    "            # Logic t√¨m file Scene JSON\n",
    "            scene_path_v1 = f'{scene_json_dirs}/{key}/{key}_{video_id}.json'\n",
    "            scene_path_v2 = f'{scene_json_dirs}/{key}/{video_id}.json'\n",
    "            final_scene_path = scene_path_v1 if os.path.exists(scene_path_v1) else (scene_path_v2 if os.path.exists(scene_path_v2) else None)\n",
    "            \n",
    "            if not final_scene_path: continue\n",
    "\n",
    "            try:\n",
    "                with open(final_scene_path) as f:\n",
    "                    scenes = json.load(f)\n",
    "                scenes = np.array([list(row) for row in scenes])\n",
    "                if len(scenes) == 0: continue\n",
    "\n",
    "                frame_numbers = get_adaptive_frames(scenes)\n",
    "                if not frame_numbers: continue\n",
    "                \n",
    "                # M·ªü Video & Extract th·∫≥ng v√†o RAM -> Zip\n",
    "                video_path = video_paths_dict[video_id]\n",
    "                vid_cap = cv2.VideoCapture(video_path)\n",
    "                \n",
    "                f_idx = 0\n",
    "                max_f = frame_numbers[-1] + 1\n",
    "                \n",
    "                for i in range(max_f):\n",
    "                    ret, frame = vid_cap.read()\n",
    "                    if not ret: break\n",
    "                    \n",
    "                    if i == frame_numbers[f_idx]:\n",
    "                        # N√©n ·∫£nh JPG v√†o RAM (Quality 80)\n",
    "                        ret_enc, buffer = cv2.imencode('.jpg', frame, [int(cv2.IMWRITE_JPEG_QUALITY), 80])\n",
    "                        \n",
    "                        if ret_enc:\n",
    "                            # C·∫•u tr√∫c: keyframes/L01/V001/0001.jpg\n",
    "                            zip_entry_name = f\"keyframes/{key}/{video_id}/{i:04d}.jpg\"\n",
    "                            zf.writestr(zip_entry_name, buffer.tobytes())\n",
    "                        \n",
    "                        f_idx += 1\n",
    "                        if f_idx >= len(frame_numbers): break\n",
    "                            \n",
    "                vid_cap.release()\n",
    "                \n",
    "            except Exception as e:\n",
    "                # print(f\"Error {video_id}: {e}\")\n",
    "                pass\n",
    "\n",
    "    if stop_processing:\n",
    "        print(\"‚ö†Ô∏è Process stopped early due to disk limits.\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Finished processing all requested batches: {TARGET_BATCHES}\")\n",
    "\n",
    "print(f\"üì¶ Closing Zip file...\")\n",
    "# Khi tho√°t kh·ªèi block 'with', file zip s·∫Ω t·ª± ƒë·ªông ƒë∆∞·ª£c finalize an to√†n\n",
    "\n",
    "final_size = os.path.getsize(final_zip_path) / (1024**3)\n",
    "print(f\"üéâ DONE! File created: {OUTPUT_ZIP_NAME} ({final_size:.2f} GB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-12-07T16:28:18.416Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# %cd /kaggle/working/\n",
    "\n",
    "# print(\"üì¶ Zipping Scenes...\")\n",
    "# !zip -rq scenes.zip scenes/\n",
    "\n",
    "# import os\n",
    "# def get_size(path):\n",
    "#     if os.path.exists(path):\n",
    "#         size = os.path.getsize(path) / (1024 * 1024 * 1024)\n",
    "#         print(f\"   -> {path}: {size:.2f} GB\")\n",
    "#     else:\n",
    "#         print(f\"   -> {path}: NOT FOUND\")\n",
    "\n",
    "# print(\"üìä Checking Output Sizes:\")\n",
    "# get_size('keyframes.zip')\n",
    "# get_size('scenes.zip')\n",
    "\n",
    "# from IPython.display import FileLink\n",
    "# print(\"\\n‚¨áÔ∏è DOWNLOAD LINKS:\")\n",
    "# display(FileLink('keyframes.zip'))\n",
    "# display(FileLink('scenes.zip'))\n",
    "\n",
    "# # Cleanup (Optional)\n",
    "# # !rm -rf scenes\n",
    "# # !rm -rf temp_keyframes"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5573261,
     "sourceId": 9216514,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30762,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
